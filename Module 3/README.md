# Introduction

Once an organisation makes a strategic decision to develop data intelligent products, the essential first step is to build the infrastructure to source, collect and store the relevant data. 

According to the collection of needs for data science, artificial intelligence (AI) and machine learning (ML) algorithms sit at the top two levels of the pyramid. This pyramid highlights the relative importance of building an effective data pipeline enabling efficient data flow, integration, storage and retrieval.

<p align="center">
<img src="assets/qed.jpg"/>

Figure 1. Rogati, M. (2017). Data Science—Hierarchy of Needs. Retrieved from https://hackernoon.com/hn-images/1*7IMev5xslc9FLxr9hHhpFw.png
</p>

Data can be collected from an external source or internal source. The structure of data coming from different sources can vary significantly. An effective data pipeline needs to consider both the source and structure of the data. Additionally, the data pipeline needs to address the ‘V’ characteristics of big data.

Thus, in a big data project, the essential first step is to identify the data sources and build the infrastructure to collect, process and store the data so that the data can be efficiently retrieved by the data scientists. In this Module, you will learn about:

- Different sources of data;
- The varying structures of data;
- The ‘data lake’ concept and its importance in a data-driven organisation;
- The principles and guidelines to follow when developing a data lake; and
- The principles and techniques of data ingestion.
